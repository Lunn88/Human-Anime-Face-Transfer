{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6747398,"sourceType":"datasetVersion","datasetId":3884958},{"sourceId":6769225,"sourceType":"datasetVersion","datasetId":3895293},{"sourceId":7036994,"sourceType":"datasetVersion","datasetId":4048474}],"dockerImageVersionId":30554,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Dataset","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\n# ffhq_data_path = \"./cartoonized_ffhq_dataset\"\nffhq_data_path = \"/kaggle/input/ffhq-20000/ffhq_dataset_20000\"\nffhq_list = []\n# anime_data_path = \"./anime_dataset\"\nanime_data_path = \"/kaggle/input/anime-dataset-16101/anime_dataset\"\nanime_list = []\n\nfor root, _, files in os.walk(ffhq_data_path):\n    for file in files:\n        if file.endswith(\".png\"):\n            img_path = os.path.join(root, file)\n            ffhq_list.append(img_path)\n\nfor root, _, files in os.walk(anime_data_path):\n    for file in files:\n        if file.endswith(\".png\"):\n            img_path = os.path.join(root, file)\n            anime_list.append(img_path)\n\nprint(\"ffhq :\", len(ffhq_list))\nprint(\"anime:\", len(anime_list))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:47:49.947442Z","iopub.execute_input":"2023-11-23T16:47:49.948064Z","iopub.status.idle":"2023-11-23T16:48:44.842110Z","shell.execute_reply.started":"2023-11-23T16:47:49.948036Z","shell.execute_reply":"2023-11-23T16:48:44.841172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms.functional import to_pil_image\nfrom IPython.display import display\nfrom torchvision.transforms.functional import to_tensor\n\nclass Dataset_(Dataset):\n    def __init__(self, ffhq_list, anime_list):\n        self.ffhq_data = ffhq_list\n        self.anime_data = anime_list\n        self.anime_len = len(anime_list)\n        \n    def __len__(self):\n        return len(self.ffhq_data)\n    \n    def __getitem__(self, index):\n        sample_f = self.ffhq_data[index]\n        sample_a = self.anime_data[index % self.anime_len]\n\n        return sample_f, sample_a\n\nbatch_size = 2\nCartoon_Dataset = Dataset_(ffhq_list, anime_list)\nCartoon_Dataloader = DataLoader(Cartoon_Dataset, batch_size=batch_size, shuffle=True)\n\nfor batch in Cartoon_Dataloader:\n    img_batch = [Image.open(img) for tuple in zip(*batch) for img in tuple]\n    display(img_batch[0])\n    display(img_batch[1])\n    display(img_batch[2])\n    display(img_batch[3])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:44.844337Z","iopub.execute_input":"2023-11-23T16:48:44.844722Z","iopub.status.idle":"2023-11-23T16:48:48.349972Z","shell.execute_reply.started":"2023-11-23T16:48:44.844688Z","shell.execute_reply":"2023-11-23T16:48:48.349048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Option","metadata":{}},{"cell_type":"code","source":"class Option():\n    def __init__(self):\n        \"\"\"Encoder\"\"\"\n        self.use_antialias = True\n        self.netE_num_downsampling_sp = 4\n        self.spatial_code_ch = 8\n        self.netE_num_downsampling_gl = 2\n        self.global_code_ch = 2048\n        self.netE_nc_steepness = 2.0\n        self.netE_scale_capacity = 1.0\n\n        \"\"\"Generator\"\"\"\n        self.num_classes = 0\n        self.netG_num_base_resnet_layers = 2\n        self.netG_use_noise = True\n        self.netG_scale_capacity = 1.0\n\n        \"\"\"Discriminator\"\"\"\n        self.crop_size = 256\n        self.netD_scale_capacity = 1.0\n        self.netPatchD_scale_capacity = 4.0\n        self.netPatchD_max_nc = 256 + 128\n        self.patch_size = 64\n        self.max_num_tiles = 8\n        self.patch_random_transformation = True\n\n        \"\"\"BaseModel\"\"\"\n        self.num_gpus = 1\n        self.checkpoints_dir = \"./checkpoints/\"\n        self.name = \"model\"\n        self.isTrain = False\n        self.pretrained_name = None\n        self.resume_iter = False\n\n        \"\"\"SwapingAE\"\"\"\n        self.lambda_R1 = 10.0\n        self.lambda_patch_R1 = 1.0\n        self.lambda_L1 = 1.0\n        self.lambda_GAN = 1.0\n        self.lambda_PatchGAN = 1.0\n        self.patch_min_scale = 1 / 8\n        self.patch_max_scale = 1 / 4\n        self.patch_num_crops = 8\n        self.patch_use_aggregation = True\n        \n        \"\"\"Optimizer\"\"\"\n        self.lr = 0.002\n        self.beta1 = 0.0\n        self.beta2 = 0.99\n        self.R1_once_every = 16\n        \n        \"\"\"Training\"\"\"\n        self.print_freq = 100\n        self.display_freq = 200\n        self.save_freq = 5000\n        \nopt = Option()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:48.351127Z","iopub.execute_input":"2023-11-23T16:48:48.351557Z","iopub.status.idle":"2023-11-23T16:48:48.362079Z","shell.execute_reply.started":"2023-11-23T16:48:48.351516Z","shell.execute_reply":"2023-11-23T16:48:48.361032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StyleGan2 Layers","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\ndef upfirdn2d_native(\n    input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n):\n    bs, ch, in_h, in_w = input.shape\n    minor = 1\n    kernel_h, kernel_w = kernel.shape\n\n    out = input.view(-1, in_h, 1, in_w, 1, minor)\n    if up_x > 1 or up_y > 1:\n        out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n\n    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n\n    if pad_x0 > 0 or pad_x1 > 0 or pad_y0 > 0 or pad_y1 > 0:\n        out = F.pad(out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)])\n\n    out = out[\n        :,\n        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n        :,\n    ]\n\n    out = out.permute(0, 3, 1, 2)\n    out = out.reshape(\n        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n    )\n\n    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n    out = F.conv2d(out, w)\n\n    out = out.reshape(\n        -1,\n        minor,\n        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n    )\n\n    out = out.permute(0, 2, 3, 1)\n    out = out[:, ::down_y, ::down_x, :]\n    out = out.view(bs, ch, out.size(1), out.size(2))\n\n    return out\n\ndef upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n    return upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:48.363256Z","iopub.execute_input":"2023-11-23T16:48:48.363648Z","iopub.status.idle":"2023-11-23T16:48:48.382337Z","shell.execute_reply.started":"2023-11-23T16:48:48.363614Z","shell.execute_reply":"2023-11-23T16:48:48.381322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\ndef fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):\n    \"\"\"\n    before bias(channels,)\n    after  bias(1, channels, 1, 1)\n    \"\"\"\n\n    dims = [1, -1] + [1] * (input.dim() - 2)\n    bias = bias.view(*dims)\n    return F.leaky_relu(input + bias, negative_slope) * scale\n\nclass FusedLeakyReLU(nn.Module):\n    def __init__(self, channel, negative_slope=0.2, scale=2 ** 0.5):\n        super().__init__()\n\n        self.bias = nn.Parameter(torch.zeros(channel))\n        self.negative_slope = negative_slope\n        self.scale = scale\n\n    def forward(self, input):\n        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:48.384807Z","iopub.execute_input":"2023-11-23T16:48:48.385109Z","iopub.status.idle":"2023-11-23T16:48:48.398507Z","shell.execute_reply.started":"2023-11-23T16:48:48.385085Z","shell.execute_reply":"2023-11-23T16:48:48.397583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\nfrom collections import OrderedDict\nimport math\n\n# from upfirdn2d import *\n# from fused_leaky_relu import *\n\ndef make_kernel(k):\n    k = torch.tensor(k, dtype = torch.float32)\n    # Create a 2D matrix using outer product\n    if k.dim() == 1:\n        k = k[None, :] * k[:, None]\n    \n    # Normalize\n    k /= k.sum()\n    return k\n\nclass Upsample(nn.Module):\n    def __init__(self, kernel, factor=2):\n        super().__init__()\n        self.factor = factor\n        kernel = make_kernel((kernel) * (factor ** 2))\n        self.register_buffer('kernel', kernel)\n        \n        p = kernel.shape[0] - factor\n\n        pad0 = (p + 1) // 2 + factor - 1\n        pad1 = p // 2\n\n        self.pad = (pad0, pad1)\n\n    def forward(self, input):\n        out = upfirdn2d(input, self.kernel, up=self.factor, down=1, pad=self.pad)\n        return out\n    \nclass Downsample(nn.Module):\n    def __init__(self, kernel, factor=2, pad=None, reflection_pad=False):\n        super().__init__()\n\n        self.factor = factor\n        kernel = make_kernel(kernel)\n        self.register_buffer('kernel', kernel)\n        self.reflection = reflection_pad\n\n        if pad is None:\n            p = kernel.shape[0] - factor\n        else:\n            p = pad\n\n        pad0 = (p + 1) // 2\n        pad1 = p // 2\n\n        self.pad = (pad0, pad1)\n\n    def forward(self, input):\n        if self.reflection:\n            input = F.pad(input, (self.pad[0], self.pad[1], self.pad[0], self.pad[1]), mode='reflect')\n            pad = (0, 0)\n        else:\n            pad = self.pad\n\n        out = upfirdn2d(input, self.kernel, up=1, down=self.factor, pad=pad)\n\n        return out\n\nclass Blur(nn.Module):\n    def __init__(self, kernel, pad, upsample_factor=1, reflection_pad=False):\n        super().__init__()\n        \n        kernel = make_kernel(kernel)\n        \n        # To match the upsampling size\n        if upsample_factor > 1:\n            kernel = kernel * (upsample_factor ** 2)\n        \n        # Register to the Blur Module buffer but won't be considered model parameters.\n        self.register_buffer('kernel', kernel)\n        \n        self.pad = pad\n        self.reflection = reflection_pad\n        if self.reflection:\n            self.reflection_pad = nn.ReflectionPad2d((pad[0], pad[1], pad[0], pad[1]))\n            self.pad = (0, 0)\n        \n    def forward(self, input):\n        if self.reflection:\n            input = self.reflection_pad(input)\n        out = upfirdn2d(input, self.kernel, pad=self.pad)\n\n        return out\n\n# Equalizing the weight initialization, prevent certain layers from dominating the learning process during training\nclass EqualConv2d(nn.Module):\n    def __init__(\n        self, in_channel, out_channel, kernel_size, stride=1, padding=0, bias=True, lr_mul=1.0\n    ):\n        super().__init__()\n        self.weight = nn.Parameter(\n            torch.randn(out_channel, in_channel, kernel_size, kernel_size)\n        )\n        self.scale = 1 / math.sqrt(in_channel * kernel_size ** 2) * lr_mul\n\n        self.stride = stride\n        self.padding = padding\n\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channel))\n        else:\n            self.bias = None\n\n    def forward(self, input):\n        out = F.conv2d(\n            input,\n            self.weight * self.scale,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        \n        return out\n    \n    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]},'\n            f' {self.weight.shape[2]}, stride={self.stride}, padding={self.padding})'\n        )\n\nclass EqualLinear(nn.Module):\n    def __init__(\n        self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1, activation=None\n    ):\n        super().__init__()\n\n        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\n\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\n\n        else:\n            self.bias = None\n\n        self.activation = activation\n\n        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n        self.lr_mul = lr_mul\n\n    def forward(self, input):\n        if self.activation:\n            if input.dim() > 2:\n                out = F.conv2d(input, self.weight[:, :, None, None] * self.scale)\n            else:\n                out = F.linear(input, self.weight * self.scale)\n            out = fused_leaky_relu(out, self.bias * self.lr_mul)\n\n        else:\n            if input.dim() > 2:\n                out = F.conv2d(input, self.weight[:, :, None, None] * self.scale,\n                               bias=self.bias * self.lr_mul\n                )\n            else:\n                out = F.linear(\n                    input, self.weight * self.scale, bias=self.bias * self.lr_mul\n                )\n\n        return out\n\n    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]})'\n        )\n            \nclass ConvLayer(nn.Sequential):\n    def __init__(\n        self,\n        in_channel,\n        out_channel,\n        kernel_size,\n        downsample=False,\n        blur_kernel=[1,3,3,1],\n        bias=True,\n        activate=True,\n        pad=None,\n        reflection_pad=False\n    ):\n        layers = []\n        \n        if downsample:\n            factor = 2\n            if pad is None:\n                pad = (len(blur_kernel) - factor) + (kernel_size - 1)\n            pad0 = (pad + 1) // 2\n            pad1 = pad // 2\n            \n            layers.append((\"Blur\", Blur(blur_kernel, pad=(pad0, pad1), reflection_pad=reflection_pad)))\n            \n            stride = 2\n            self.padding = 0\n        else:\n            stride = 1\n            self.padding = kernel_size // 2 if pad is None else pad\n            if reflection_pad:\n                layers.append((\"RefPad\", nn.ReflectionPad2d(self.padding)))\n                self.padding = 0\n        \n        layers.append((\"Conv\",\n                       EqualConv2d(\n                           in_channel,\n                           out_channel,\n                           kernel_size,\n                           padding=self.padding,\n                           stride=stride,\n                           bias=bias and not activate,\n                       ))\n        )\n\n        if activate:\n            if bias:\n                layers.append((\"Act\", FusedLeakyReLU(out_channel)))\n            else:\n                layers.append((\"Act\", ScaledLeakyReLU(0.2)))\n\n        super().__init__(OrderedDict(layers))\n\n    def forward(self, x):\n        out = super().forward(x)\n        return out\n    \nclass ResBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, blur_kernel=[1,3,3,1], reflection_pad=False, pad=None, downsample=True):\n        super().__init__()\n        self.conv1 = ConvLayer(in_channel, in_channel, 3, reflection_pad=reflection_pad, pad=pad)\n        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=downsample, blur_kernel=blur_kernel, reflection_pad=reflection_pad, pad=pad)\n        self.skip = ConvLayer(\n            in_channel, out_channel, 1, downsample=downsample, blur_kernel=blur_kernel, activate=False, bias=False\n        )\n        \n    def forward(self, input):\n        out = self.conv1(input)\n        out = self.conv2(out)\n        skip = self.skip(input)\n        \n        return (out + skip) / math.sqrt(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:48.399737Z","iopub.execute_input":"2023-11-23T16:48:48.400038Z","iopub.status.idle":"2023-11-23T16:48:48.437419Z","shell.execute_reply.started":"2023-11-23T16:48:48.400006Z","shell.execute_reply":"2023-11-23T16:48:48.436599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Network","metadata":{}},{"cell_type":"code","source":"class BaseNetwork(torch.nn.Module):\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n        \n    def print_architecture(self, verbose=False):\n        name = type(self).__name__\n        result = '-------------------%s---------------------\\n' % name\n        total_num_params = 0\n        for i, (name, child) in enumerate(self.named_children()):\n            num_params = sum([p.numel() for p in child.parameters()])\n            total_num_params += num_params\n            if verbose:\n                result += \"%s: %3.3fM\\n\" % (name, (num_params / 1e6))\n            for i, (name, grandchild) in enumerate(child.named_children()):\n                num_params = sum([p.numel() for p in grandchild.parameters()])\n                if verbose:\n                    result += \"\\t%s: %3.3fM\\n\" % (name, (num_params / 1e6))\n        result += '[Network %s] Total number of parameters : %.3f M\\n' % (name, total_num_params / 1e6)\n        result += '-----------------------------------------------\\n'\n        print(result)\n        \n    def set_requires_grad(self, requires_grad):\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def collect_parameters(self, name):\n        params = []\n        for m in self.modules():\n            if type(m).__name__ == name:\n                params += list(m.parameters())\n        return params\n\n    def fix_and_gather_noise_parameters(self):\n        params = []\n        device = next(self.parameters()).device\n        for m in self.modules():\n            if type(m).__name__ == \"NoiseInjection\":\n                assert m.image_size is not None, \"One forward call should be made to determine size of noise parameters\"\n                m.fixed_noise = torch.nn.Parameter(torch.randn(m.image_size[0], 1, m.image_size[2], m.image_size[3], device=device))\n                params.append(m.fixed_noise)\n        return params\n\n    def remove_noise_parameters(self, name):\n        for m in self.modules():\n            if type(m).__name__ == \"NoiseInjection\":\n                m.fixed_noise = None\n\n    def forward(self, x):\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:48.438491Z","iopub.execute_input":"2023-11-23T16:48:48.438768Z","iopub.status.idle":"2023-11-23T16:48:48.453526Z","shell.execute_reply.started":"2023-11-23T16:48:48.438746Z","shell.execute_reply":"2023-11-23T16:48:48.452785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport torch\nimport numbers\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nfrom PIL import Image\nimport os\nimport importlib\nimport argparse\nfrom argparse import Namespace\nfrom sklearn.decomposition import PCA as PCA\n\ndef normalize(v):\n    if type(v) == list:\n        return [normalize(vv) for vv in v]\n\n    return v * torch.rsqrt((torch.sum(v ** 2, dim=1, keepdim=True) + 1e-8))\n\nclass RandomSpatialTransformer:\n    def __init__(self, opt, bs):\n        self.opt = opt\n        #self.resample_transformation(bs)\n\n    def create_affine_transformation(self, ref, rot, sx, sy, tx, ty):\n        return torch.stack([-ref * sx * torch.cos(rot), -sy * torch.sin(rot), tx,\n                            -ref * sx * torch.sin(rot), sy * torch.cos(rot), ty], axis=1)\n\n    def resample_transformation(self, bs, device, reflection=None, rotation=None, scale=None, translation=None):\n        dev = device\n        zero = torch.zeros((bs), device=dev)\n        if reflection is None:\n            #if \"ref\" in self.opt.random_transformation_mode:\n            ref = torch.round(torch.rand((bs), device=dev)) * 2 - 1\n            #else:\n            #    ref = 1.0\n        else:\n            ref = reflection\n\n        if rotation is None:\n            #if \"rot\" in self.opt.random_transformation_mode:\n            max_rotation = 30 * math.pi / 180\n            rot = torch.rand((bs), device=dev) * (2 * max_rotation) - max_rotation\n            #else:\n            #    rot = 0.0\n        else:\n            rot = rotation\n\n        if scale is None:\n            #if \"scale\" in self.opt.random_transformation_mode:\n            min_scale = 1.0\n            max_scale = 1.0\n            sx = torch.rand((bs), device=dev) * (max_scale - min_scale) + min_scale\n            sy = torch.rand((bs), device=dev) * (max_scale - min_scale) + min_scale\n            #else:\n            #    sx, sy = 1.0, 1.0\n        else:\n            sx, sy = scale\n\n        tx, ty = zero, zero\n\n        A = torch.stack([ref * sx * torch.cos(rot), -sy * torch.sin(rot), tx,\n                         ref * sx * torch.sin(rot), sy * torch.cos(rot), ty], axis=1)\n        return A.view(bs, 2, 3)\n\n    def forward_transform(self, x, size):\n        if type(x) == list:\n            return [self.forward_transform(xx) for xx in x]\n\n        affine_param = self.resample_transformation(x.size(0), x.device)\n        affine_grid = F.affine_grid(affine_param, (x.size(0), x.size(1), size[0], size[1]), align_corners=False)\n        x = F.grid_sample(x, affine_grid, padding_mode='reflection', align_corners=False)\n\n        return x\n    \ndef resize2d_tensor(x, size_or_tensor_of_size):\n    if torch.is_tensor(size_or_tensor_of_size):\n        size = size_or_tensor_of_size.size()\n    elif isinstance(size_or_tensor_of_size, np.ndarray):\n        size = size_or_tensor_of_size.shape\n    else:\n        size = size_or_tensor_of_size\n\n    if isinstance(size, tuple) or isinstance(size, list):\n        return F.interpolate(x, size[-2:],\n                             mode='bilinear', align_corners=False)\n    else:\n        raise ValueError(\"%s is unrecognized\" % str(type(size)))\n    \ndef visualize_spatial_code(sp):\n    device = sp.device\n    #sp = (sp - sp.min()) / (sp.max() - sp.min() + 1e-7)\n    if sp.size(1) <= 2:\n        sp = sp.repeat([1, 3, 1, 1])[:, :3, :, :]\n    if sp.size(1) == 3:\n        pass\n    else:\n        sp = sp.detach().cpu().numpy()\n        X = np.transpose(sp, (0, 2, 3, 1))\n        B, H, W = X.shape[0], X.shape[1], X.shape[2]\n        X = np.reshape(X, (-1, X.shape[3]))\n        X = X - X.mean(axis=0, keepdims=True)\n        try:\n            Z = PCA(3).fit_transform(X)\n        except ValueError:\n            print(\"Running PCA on the structure code has failed.\")\n            print(\"This is likely a bug of scikit-learn in version 0.18.1.\")\n            print(\"https://stackoverflow.com/a/42764378\")\n            print(\"The visualization of the structure code on visdom won't work.\")\n            return torch.zeros(B, 3, H, W, device=device)\n        sp = np.transpose(np.reshape(Z, (B, H, W, -1)), (0, 3, 1, 2))\n        sp = (sp - sp.min()) / (sp.max() - sp.min()) * 2 - 1\n        sp = torch.from_numpy(sp).to(device)\n    return sp\n\ndef to_numpy(metric_dict):\n    new_dict = {}\n    for k, v in metric_dict.items():\n        if \"numpy\" not in str(type(v)):\n            v = v.detach().cpu().mean().numpy()\n        new_dict[k] = v\n    return new_dict","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:48.454999Z","iopub.execute_input":"2023-11-23T16:48:48.455491Z","iopub.status.idle":"2023-11-23T16:48:49.063488Z","shell.execute_reply.started":"2023-11-23T16:48:48.455459Z","shell.execute_reply":"2023-11-23T16:48:49.062519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# from util import *\n\nclass ToSpatialCode(torch.nn.Module):\n    def __init__(self, inch, outch, scale):\n        super().__init__()\n        hiddench = inch // 2\n        self.conv1 = ConvLayer(inch, hiddench, 1, activate=True, bias=True)\n        self.conv2 = ConvLayer(hiddench, outch, 1, activate=False, bias=True)\n        self.scale = scale\n        self.upsample = Upsample([1, 3, 3, 1], 2)\n        self.blur = Blur([1, 3, 3, 1], pad=(2, 1))\n        self.register_buffer('kernel', make_kernel([1, 3, 3, 1]))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        for i in range(int(np.log2(self.scale))):\n            x = self.upsample(x)\n        return x\n        \nclass Encoder(BaseNetwork):\n    def __init__(self, opt):\n        super().__init__(opt)\n        \n        blur_kernel = [1, 2, 1] if self.opt.use_antialias else [1]\n        self.add_module(\"FromRGB\", ConvLayer(3, self.nc(0), 1))\n        \n        self.DownToSpatialCode = nn.Sequential()\n        for i in range(self.opt.netE_num_downsampling_sp):\n            self.DownToSpatialCode.add_module(\n                \"ResBlockDownBy%d\" % (2 ** i),\n                ResBlock(self.nc(i), self.nc(i + 1), blur_kernel, reflection_pad=True)\n            )\n            \n        nchannels = self.nc(self.opt.netE_num_downsampling_sp)\n        self.add_module(\n            \"ToSpatialCode\",\n            nn.Sequential(\n                ConvLayer(nchannels, nchannels, 1, activate=True, bias=True),\n                ConvLayer(nchannels, self.opt.spatial_code_ch, kernel_size=1,\n                          activate=False, bias=True)\n            )\n        )\n        \n        self.DownToGlobalCode = nn.Sequential()\n        for i in range(self.opt.netE_num_downsampling_gl):\n            idx_from_beginning = self.opt.netE_num_downsampling_sp + i\n            self.DownToGlobalCode.add_module(\n                \"ConvLayerDownBy%d\" % (2 ** idx_from_beginning),\n                ConvLayer(self.nc(idx_from_beginning),\n                          self.nc(idx_from_beginning + 1), kernel_size=3,\n                          blur_kernel=[1], downsample=True, pad=0)\n            )\n            \n        nchannels = self.nc(self.opt.netE_num_downsampling_sp +\n                            self.opt.netE_num_downsampling_gl)\n        self.add_module(\n            \"ToGlobalCode\",\n            nn.Sequential(\n                EqualLinear(nchannels, self.opt.global_code_ch)\n            )\n        )\n        \n    def nc(self, idx):\n        nc = self.opt.netE_nc_steepness ** (5 + idx)\n        nc = nc * self.opt.netE_scale_capacity\n        nc = min(self.opt.global_code_ch, int(round(nc)))\n        return round(nc)\n    \n    def forward(self, x, extract_features=False):\n        x = self.FromRGB(x)\n        midpoint = self.DownToSpatialCode(x)\n        sp = self.ToSpatialCode(midpoint)\n        \n        if extract_features:\n            padded_midpoint = F.pad(midpoint, (1, 0, 1, 0), mode='reflect')\n            feature = self.DownToGlobalCode[0](padded_midpoint)\n            assert feature.size(2) == sp.size(2) // 2 and feature.size(3) == sp.size(3) // 2\n            feature = F.interpolate(\n                feature, size=(7, 7), mode='bilinear', align_corners=False)\n            \n        x = self.DownToGlobalCode(midpoint)\n        x = x.mean(dim=(2, 3))\n        gl = self.ToGlobalCode(x)\n        sp = normalize(sp)\n        gl = normalize(gl)\n        if extract_features:\n            return sp, gl, feature\n        else:\n            return sp, gl","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.064803Z","iopub.execute_input":"2023-11-23T16:48:49.065089Z","iopub.status.idle":"2023-11-23T16:48:49.085281Z","shell.execute_reply.started":"2023-11-23T16:48:49.065064Z","shell.execute_reply":"2023-11-23T16:48:49.084258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StyleGan2 Layers","metadata":{}},{"cell_type":"code","source":"class PixelNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        return input * torch.rsqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n\nclass ModulatedConv2d(nn.Module):\n    def __init__(\n        self,\n        in_channel,\n        out_channel,\n        kernel_size,\n        style_dim,\n        demodulate=True,\n        upsample=False,\n        downsample=False,\n        blur_kernel=[1, 3, 3, 1],\n    ):\n        super().__init__()\n\n        self.eps = 1e-8\n        self.kernel_size = kernel_size\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n        self.upsample = upsample\n        self.downsample = downsample\n\n        if upsample:\n            factor = 2\n            p = (len(blur_kernel) - factor) - (kernel_size - 1)\n            pad0 = (p + 1) // 2 + factor - 1\n            pad1 = p // 2 + 1\n\n            self.blur = Blur(blur_kernel, pad=(pad0, pad1), upsample_factor=factor)\n\n        if downsample:\n            factor = 2\n            p = (len(blur_kernel) - factor) + (kernel_size - 1)\n            pad0 = (p + 1) // 2\n            pad1 = p // 2\n\n            self.blur = Blur(blur_kernel, pad=(pad0, pad1))\n\n        fan_in = in_channel * kernel_size ** 2\n        self.scale = 1 / math.sqrt(fan_in)\n        self.padding = kernel_size // 2\n\n        self.weight = nn.Parameter(\n            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)\n        )\n\n        self.modulation = EqualLinear(style_dim, in_channel, bias_init=1)\n\n        self.demodulate = demodulate\n        self.new_demodulation = True\n\n    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}({self.in_channel}, {self.out_channel}, {self.kernel_size}, '\n            f'upsample={self.upsample}, downsample={self.downsample})'\n        )\n\n    def forward(self, input, style):\n        batch, in_channel, height, width = input.shape\n\n        if style.dim() > 2:\n            style = F.interpolate(style, size=(input.size(2), input.size(3)), mode='bilinear', align_corners=False)\n            style = self.modulation(style).unsqueeze(1)\n            if self.demodulate:\n                style = style * torch.rsqrt(style.pow(2).mean([2], keepdim=True) + 1e-8)\n            input = input * style\n            weight = self.scale * self.weight\n            weight = weight.repeat(batch, 1, 1, 1, 1)\n        else:\n            style = style.view(batch, style.size(1))\n            style = self.modulation(style).view(batch, 1, in_channel, 1, 1)\n            if self.new_demodulation:\n                style = style[:, 0, :, :, :]\n                if self.demodulate:\n                    style = style * torch.rsqrt(style.pow(2).mean([1], keepdim=True) + 1e-8)\n                input = input * style\n                weight = self.scale * self.weight\n                weight = weight.repeat(batch, 1, 1, 1, 1)\n            else:\n                weight = self.scale * self.weight * style\n\n        if self.demodulate:\n            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n            weight = weight * demod.view(batch, self.out_channel, 1, 1, 1)\n\n        weight = weight.view(\n            batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size\n        )\n\n        if self.upsample:\n            input = input.view(1, batch * in_channel, height, width)\n            weight = weight.view(\n                batch, self.out_channel, in_channel, self.kernel_size, self.kernel_size\n            )\n            weight = weight.transpose(1, 2).reshape(\n                batch * in_channel, self.out_channel, self.kernel_size, self.kernel_size\n            )\n            out = F.conv_transpose2d(input, weight, padding=0, stride=2, groups=batch)\n            _, _, height, width = out.shape\n            out = out.view(batch, self.out_channel, height, width)\n            out = self.blur(out)\n\n        elif self.downsample:\n            input = self.blur(input)\n            _, _, height, width = input.shape\n            input = input.view(1, batch * in_channel, height, width)\n            out = F.conv2d(input, weight, padding=0, stride=2, groups=batch)\n            _, _, height, width = out.shape\n            out = out.view(batch, self.out_channel, height, width)\n\n        else:\n            input = input.view(1, batch * in_channel, height, width)\n            out = F.conv2d(input, weight, padding=self.padding, groups=batch)\n            _, _, height, width = out.shape\n            out = out.view(batch, self.out_channel, height, width)\n\n        return out\n            \nclass NoiseInjection(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.weight = nn.Parameter(torch.zeros(1))\n        self.fixed_noise = None\n        self.image_size = None\n\n    def forward(self, image, noise=None):\n        if self.image_size is None:\n            self.image_size = image.shape\n\n        if noise is None and self.fixed_noise is None:\n            batch, _, height, width = image.shape\n            noise = image.new_empty(batch, 1, height, width).normal_()\n        elif self.fixed_noise is not None:\n            noise = self.fixed_noise\n            # to avoid error when generating thumbnails in demo\n            if image.size(2) != noise.size(2) or image.size(3) != noise.size(3):\n                noise = F.interpolate(noise, image.shape[2:], mode=\"nearest\")\n        else:\n            pass  # use the passed noise\n\n        return image + self.weight * noise\n\nclass StyledConv(nn.Module):\n    def __init__(\n        self,\n        in_channel,\n        out_channel,\n        kernel_size,\n        style_dim,\n        upsample=False,\n        blur_kernel=[1, 3, 3, 1],\n        demodulate=True,\n        use_noise=True,\n        lr_mul=1.0,\n    ):\n        super().__init__()\n\n        self.conv = ModulatedConv2d(\n            in_channel,\n            out_channel,\n            kernel_size,\n            style_dim,\n            upsample=upsample,\n            blur_kernel=blur_kernel,\n            demodulate=demodulate,\n        )\n\n        self.use_noise = use_noise\n        self.noise = NoiseInjection()\n        # self.bias = nn.Parameter(torch.zeros(1, out_channel, 1, 1))\n        # self.activate = ScaledLeakyReLU(0.2)\n        self.activate = FusedLeakyReLU(out_channel)\n\n    def forward(self, input, style, noise=None):\n        out = self.conv(input, style)\n        if self.use_noise:\n            out = self.noise(out, noise=noise)\n        # out = out + self.bias\n        out = self.activate(out)\n\n        return out\n    \nclass ToRGB(nn.Module):\n    def __init__(self, in_channel, style_dim, upsample=True, blur_kernel=[1, 3, 3, 1]):\n        super().__init__()\n\n        if upsample:\n            self.upsample = Upsample(blur_kernel)\n\n        self.conv = ModulatedConv2d(in_channel, 3, 1, style_dim, demodulate=False)\n        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n\n    def forward(self, input, style, skip=None):\n        out = self.conv(input, style)\n        out = out + self.bias\n\n        if skip is not None:\n            skip = self.upsample(skip)\n\n            out = out + skip\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.086603Z","iopub.execute_input":"2023-11-23T16:48:49.086866Z","iopub.status.idle":"2023-11-23T16:48:49.122183Z","shell.execute_reply.started":"2023-11-23T16:48:49.086843Z","shell.execute_reply":"2023-11-23T16:48:49.121287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"code","source":"class UpsamplingBlock(torch.nn.Module):\n    def __init__(self, inch, outch, styledim,\n                 blur_kernel=[1, 3, 3, 1], use_noise=False):\n        super().__init__()\n        self.inch, self.outch, self.styledim = inch, outch, styledim\n        self.conv1 = StyledConv(inch, outch, 3, styledim, upsample=True,\n                                blur_kernel=blur_kernel, use_noise=use_noise)\n        self.conv2 = StyledConv(outch, outch, 3, styledim, upsample=False,\n                                use_noise=use_noise)\n\n    def forward(self, x, style):\n        return self.conv2(self.conv1(x, style), style)\n    \nclass ResolutionPreservingResnetBlock(torch.nn.Module):\n    def __init__(self, opt, inch, outch, styledim):\n        super().__init__()\n        self.conv1 = StyledConv(inch, outch, 3, styledim, upsample=False)\n        self.conv2 = StyledConv(outch, outch, 3, styledim, upsample=False)\n        if inch != outch:\n            self.skip = ConvLayer(inch, outch, 1, activate=False, bias=False)\n        else:\n            self.skip = torch.nn.Identity()\n\n    def forward(self, x, style):\n        skip = self.skip(x)\n        res = self.conv2(self.conv1(x, style), style)\n        return (skip + res) / math.sqrt(2)\n    \nclass UpsamplingResnetBlock(torch.nn.Module):\n    def __init__(self, inch, outch, styledim, blur_kernel=[1, 3, 3, 1], use_noise=False):\n        super().__init__()\n        self.inch, self.outch, self.styledim = inch, outch, styledim\n        self.conv1 = StyledConv(inch, outch, 3, styledim, upsample=True, blur_kernel=blur_kernel, use_noise=use_noise)\n        self.conv2 = StyledConv(outch, outch, 3, styledim, upsample=False, use_noise=use_noise)\n        if inch != outch:\n            self.skip = ConvLayer(inch, outch, 1, activate=True, bias=True)\n        else:\n            self.skip = torch.nn.Identity()\n\n    def forward(self, x, style):\n        skip = F.interpolate(self.skip(x), scale_factor=2, mode='bilinear', align_corners=False)\n        res = self.conv2(self.conv1(x, style), style)\n        return (skip + res) / math.sqrt(2)\n    \nclass GeneratorModulation(torch.nn.Module):\n    def __init__(self, styledim, outch):\n        super().__init__()\n        self.scale = EqualLinear(styledim, outch)\n        self.bias = EqualLinear(styledim, outch)\n\n    def forward(self, x, style):\n        if style.ndimension() <= 2:\n            return x * (1 * self.scale(style)[:, :, None, None]) + self.bias(style)[:, :, None, None]\n        else:\n            style = F.interpolate(style, size=(x.size(2), x.size(3)), mode='bilinear', align_corners=False)\n            return x * (1 * self.scale(style)) + self.bias(style)\n\nclass Generator(BaseNetwork):\n    def __init__(self, opt):\n        super().__init__(opt)\n        num_upsamplings = opt.netE_num_downsampling_sp\n        blur_kernel = [1, 3, 3, 1] if opt.use_antialias else [1]\n\n        self.global_code_ch = opt.global_code_ch + opt.num_classes\n\n        self.add_module(\n            \"SpatialCodeModulation\",\n            GeneratorModulation(self.global_code_ch, opt.spatial_code_ch))\n\n        in_channel = opt.spatial_code_ch\n        for i in range(opt.netG_num_base_resnet_layers):\n            # gradually increase the number of channels\n            out_channel = (i + 1) / opt.netG_num_base_resnet_layers * self.nf(0)\n            out_channel = max(opt.spatial_code_ch, round(out_channel))\n            layer_name = \"HeadResnetBlock%d\" % i\n            new_layer = ResolutionPreservingResnetBlock(\n                opt, in_channel, out_channel, self.global_code_ch)\n            self.add_module(layer_name, new_layer)\n            in_channel = out_channel\n\n        for j in range(num_upsamplings):\n            out_channel = self.nf(j + 1)\n            layer_name = \"UpsamplingResBlock%d\" % (2 ** (4 + j))\n            new_layer = UpsamplingResnetBlock(\n                in_channel, out_channel, self.global_code_ch,\n                blur_kernel, opt.netG_use_noise)\n            self.add_module(layer_name, new_layer)\n            in_channel = out_channel\n\n        last_layer = ToRGB(out_channel, self.global_code_ch,\n                           blur_kernel=blur_kernel)\n        self.add_module(\"ToRGB\", last_layer)\n\n    def nf(self, num_up):\n        ch = 128 * (2 ** (self.opt.netE_num_downsampling_sp - num_up))\n        ch = int(min(512, ch) * self.opt.netG_scale_capacity)\n        return ch\n\n    def forward(self, spatial_code, global_code):\n        spatial_code = normalize(spatial_code)\n        global_code = normalize(global_code)\n\n        x = self.SpatialCodeModulation(spatial_code, global_code)\n        for i in range(self.opt.netG_num_base_resnet_layers):\n            resblock = getattr(self, \"HeadResnetBlock%d\" % i)\n            x = resblock(x, global_code)\n\n        for j in range(self.opt.netE_num_downsampling_sp):\n            key_name = 2 ** (4 + j)\n            upsampling_layer = getattr(self, \"UpsamplingResBlock%d\" % key_name)\n            x = upsampling_layer(x, global_code)\n        rgb = self.ToRGB(x, global_code, None)\n\n        return rgb","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.123411Z","iopub.execute_input":"2023-11-23T16:48:49.123699Z","iopub.status.idle":"2023-11-23T16:48:49.149677Z","shell.execute_reply.started":"2023-11-23T16:48:49.123676Z","shell.execute_reply":"2023-11-23T16:48:49.148825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StyleGan2 Layer","metadata":{}},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, blur_kernel=[1, 3, 3, 1], reflection_pad=False, pad=None, downsample=True):\n        super().__init__()\n\n        self.conv1 = ConvLayer(in_channel, in_channel, 3, reflection_pad=reflection_pad, pad=pad)\n        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=downsample, blur_kernel=blur_kernel, reflection_pad=reflection_pad, pad=pad)\n\n        self.skip = ConvLayer(\n            in_channel, out_channel, 1, downsample=downsample, blur_kernel=blur_kernel, activate=False, bias=False\n        )\n\n    def forward(self, input):\n        #print(\"before first resnet layeer, \", input.shape)\n        out = self.conv1(input)\n        #print(\"after first resnet layer, \", out.shape)\n        out = self.conv2(out)\n        #print(\"after second resnet layer, \", out.shape)\n\n        skip = self.skip(input)\n        out = (out + skip) / math.sqrt(2)\n\n        return out\n\nclass BaseDiscriminator(nn.Module):\n    def __init__(self, size, channel_multiplier=2, blur_kernel=[1, 3, 3, 1]):\n        super().__init__()\n\n        channels = {\n            4: 512,\n            8: 512,\n            16: min(512, int(512 * channel_multiplier)),\n            32: min(512, int(512 * channel_multiplier)),\n            64: int(256 * channel_multiplier),\n            128: int(128 * channel_multiplier),\n            256: int(64 * channel_multiplier),\n            512: int(32 * channel_multiplier),\n            1024: int(16 * channel_multiplier),\n        }\n\n        original_size = size\n        size = 2 ** int(round(math.log(size, 2)))\n        convs = [('0', ConvLayer(3, channels[size], 1))]\n\n        log_size = int(math.log(size, 2))\n\n        in_channel = channels[size]\n\n        for i in range(log_size, 2, -1):\n            out_channel = channels[2 ** (i - 1)]\n            layer_name = str(9 - i) if i <= 8 else \"%dx%d\" % (2 ** i, 2 ** i)\n            convs.append((layer_name, ResBlock(in_channel, out_channel, blur_kernel)))\n\n            in_channel = out_channel\n\n        self.convs = nn.Sequential(OrderedDict(convs))\n\n        #self.stddev_group = 4\n        #self.stddev_feat = 1\n\n        self.final_conv = ConvLayer(in_channel, channels[4], 3)\n\n        side_length = int(4 * original_size / size)\n\n        self.final_linear = nn.Sequential(\n            EqualLinear(channels[4] * (side_length ** 2), channels[4], activation='fused_lrelu'),\n            EqualLinear(channels[4], 1),\n        )\n\n    def forward(self, input):\n        out = self.convs(input)\n\n        batch, channel, height, width = out.shape\n        \n        #group = min(batch, self.stddev_group)\n        #stddev = out.view(\n        #    group, -1, self.stddev_feat, channel // self.stddev_feat, height, width\n        #)\n        #stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)\n        #stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)\n        #stddev = stddev.repeat(group, 1, height, width)\n        #out = torch.cat([out, stddev], 1)\n\n        out = self.final_conv(out)\n        out = out.view(batch, -1)\n        out = self.final_linear(out)\n\n        return out\n\n    def get_features(self, input):\n        return self.final_conv(self.convs(input))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.150775Z","iopub.execute_input":"2023-11-23T16:48:49.151038Z","iopub.status.idle":"2023-11-23T16:48:49.168412Z","shell.execute_reply.started":"2023-11-23T16:48:49.151015Z","shell.execute_reply":"2023-11-23T16:48:49.167659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(BaseNetwork):\n    def __init__(self, opt):\n        super().__init__(opt)\n        self.stylegan2_D = BaseDiscriminator(\n            opt.crop_size,\n            2.0 * opt.netD_scale_capacity,\n            blur_kernel=[1, 3, 3, 1] if self.opt.use_antialias else [1]\n        )\n\n    def forward(self, x):\n        pred = self.stylegan2_D(x)\n        return pred\n\n    def get_features(self, x):\n        return self.stylegan2_D.get_features(x)\n\n    def get_pred_from_features(self, feat, label):\n        assert label is None\n        feat = feat.flatten(1)\n        out = self.stylegan2_D.final_linear(feat)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.169576Z","iopub.execute_input":"2023-11-23T16:48:49.170339Z","iopub.status.idle":"2023-11-23T16:48:49.183383Z","shell.execute_reply.started":"2023-11-23T16:48:49.170314Z","shell.execute_reply":"2023-11-23T16:48:49.182599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Patch discriminator","metadata":{}},{"cell_type":"code","source":"class BasePatchDiscriminator(BaseNetwork):\n    def __init__(self, opt):\n        super().__init__(opt)\n        #self.visdom = util.Visualizer(opt)\n\n    def needs_regularization(self):\n        return False\n\n    def extract_features(self, patches):\n        raise NotImplementedError()\n\n    def discriminate_features(self, feature1, feature2):\n        raise NotImplementedError()\n\n    def apply_random_transformation(self, patches):\n        B, ntiles, C, H, W = patches.size()\n        patches = patches.view(B * ntiles, C, H, W)\n        before = patches\n        transformer = RandomSpatialTransformer(self.opt, B * ntiles)\n        patches = transformer.forward_transform(patches, (self.opt.patch_size, self.opt.patch_size))\n        #self.visdom.display_current_results({'before': before,\n        #                                     'after': patches}, 0, save_result=False)\n        return patches.view(B, ntiles, C, H, W)\n\n    def sample_patches_old(self, img, indices):\n        B, C, H, W = img.size()\n        s = self.opt.patch_size\n        if H % s > 0 or W % s > 0:\n            y_offset = torch.randint(H % s, (), device=img.device)\n            x_offset = torch.randint(W % s, (), device=img.device)\n            img = img[:, :,\n                      y_offset:y_offset + s * (H // s),\n                      x_offset:x_offset + s * (W // s)]\n        img = img.view(B, C, H//s, s, W//s, s)\n        ntiles = (H // s) * (W // s)\n        tiles = img.permute(0, 2, 4, 1, 3, 5).reshape(B, ntiles, C, s, s)\n        if indices is None:\n            indices = torch.randperm(ntiles, device=img.device)[:self.opt.max_num_tiles]\n            return self.apply_random_transformation(tiles[:, indices]), indices\n        else:\n            return self.apply_random_transformation(tiles[:, indices])\n\n    def forward(self, real, fake, fake_only=False):\n        assert real is not None\n        real_patches, patch_ids = self.sample_patches(real, None)\n        if fake is None:\n            real_patches.requires_grad_()\n        real_feat = self.extract_features(real_patches)\n\n        bs = real.size(0)\n        if fake is None or not fake_only:\n            pred_real = self.discriminate_features(\n                real_feat,\n                torch.roll(real_feat, 1, 1))\n            pred_real = pred_real.view(bs, -1)\n\n\n        if fake is not None:\n            fake_patches = self.sample_patches(fake, patch_ids)\n            #self.visualizer.display_current_results({'real_A': real_patches[0],\n            #                                         'real_B': torch.roll(fake_patches, 1, 1)[0]}, 0, False, max_num_images=16)\n            fake_feat = self.extract_features(fake_patches)\n            pred_fake = self.discriminate_features(\n                real_feat,\n                torch.roll(fake_feat, 1, 1))\n            pred_fake = pred_fake.view(bs, -1)\n\n        if fake is None:\n            return pred_real, real_patches\n        elif fake_only:\n            return pred_fake\n        else:\n            return pred_real, pred_fake\n        \nclass PatchDiscriminator(BasePatchDiscriminator):\n\n    def __init__(self, opt):\n        super().__init__(opt)\n        channel_multiplier = self.opt.netPatchD_scale_capacity\n        size = self.opt.patch_size\n        channels = {\n            4: min(self.opt.netPatchD_max_nc, int(256 * channel_multiplier)),\n            8: min(self.opt.netPatchD_max_nc, int(128 * channel_multiplier)),\n            16: min(self.opt.netPatchD_max_nc, int(64 * channel_multiplier)),\n            32: int(32 * channel_multiplier),\n            64: int(16 * channel_multiplier),\n            128: int(8 * channel_multiplier),\n            256: int(4 * channel_multiplier),\n        }\n\n        log_size = int(math.ceil(math.log(size, 2)))\n\n        in_channel = channels[2 ** log_size]\n\n        blur_kernel = [1, 3, 3, 1] if self.opt.use_antialias else [1]\n\n        convs = [('0', ConvLayer(3, in_channel, 3))]\n\n        for i in range(log_size, 2, -1):\n            out_channel = channels[2 ** (i - 1)]\n\n            layer_name = str(7 - i) if i <= 6 else \"%dx%d\" % (2 ** i, 2 ** i)\n            convs.append((layer_name, ResBlock(in_channel, out_channel, blur_kernel)))\n\n            in_channel = out_channel\n\n        convs.append(('5', ResBlock(in_channel, self.opt.netPatchD_max_nc * 2, downsample=False)))\n        convs.append(('6', ConvLayer(self.opt.netPatchD_max_nc * 2, self.opt.netPatchD_max_nc, 3, pad=0)))\n\n        self.convs = nn.Sequential(OrderedDict(convs))\n\n        out_dim = 1\n\n        pairlinear1 = EqualLinear(channels[4] * 2 * 2 * 2, 2048, activation='fused_lrelu')\n        pairlinear2 = EqualLinear(2048, 2048, activation='fused_lrelu')\n        pairlinear3 = EqualLinear(2048, 1024, activation='fused_lrelu')\n        pairlinear4 = EqualLinear(1024, out_dim)\n        self.pairlinear = nn.Sequential(pairlinear1, pairlinear2, pairlinear3, pairlinear4)\n\n    def extract_features(self, patches, aggregate=False):\n        if patches.ndim == 5:\n            B, T, C, H, W = patches.size()\n            flattened_patches = patches.flatten(0, 1)\n        else:\n            B, C, H, W = patches.size()\n            T = patches.size(1)\n            flattened_patches = patches\n        features = self.convs(flattened_patches)\n        features = features.view(B, T, features.size(1), features.size(2), features.size(3))\n        if aggregate:\n            features = features.mean(1, keepdim=True).expand(-1, T, -1, -1, -1)\n        return features.flatten(0, 1)\n\n    def extract_layerwise_features(self, image):\n        feats = [image]\n        for m in self.convs:\n            feats.append(m(feats[-1]))\n\n        return feats\n\n    def discriminate_features(self, feature1, feature2):\n        feature1 = feature1.flatten(1)\n        feature2 = feature2.flatten(1)\n        out = self.pairlinear(torch.cat([feature1, feature2], dim=1))\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.187242Z","iopub.execute_input":"2023-11-23T16:48:49.187506Z","iopub.status.idle":"2023-11-23T16:48:49.216174Z","shell.execute_reply.started":"2023-11-23T16:48:49.187483Z","shell.execute_reply":"2023-11-23T16:48:49.215208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_random_crop(x, target_size, scale_range, num_crops=1, return_rect=False):\n    # build grid\n    B = x.size(0) * num_crops\n    flip = torch.round(torch.rand(B, 1, 1, 1, device=x.device)) * 2 - 1.0\n    unit_grid_x = torch.linspace(-1.0, 1.0, target_size, device=x.device)[np.newaxis, np.newaxis, :, np.newaxis].repeat(B, target_size, 1, 1)\n    unit_grid_y = unit_grid_x.transpose(1, 2)\n    unit_grid = torch.cat([unit_grid_x * flip, unit_grid_y], dim=3)\n\n\n    #crops = []\n    x = x.unsqueeze(1).expand(-1, num_crops, -1, -1, -1).flatten(0, 1)\n    #for i in range(num_crops):\n    scale = torch.rand(B, 1, 1, 2, device=x.device) * (scale_range[1] - scale_range[0]) + scale_range[0]\n    offset = (torch.rand(B, 1, 1, 2, device=x.device) * 2 - 1) * (1 - scale)\n    sampling_grid = unit_grid * scale + offset\n    crop = F.grid_sample(x, sampling_grid, align_corners=False)\n    #crops.append(crop)\n    #crop = torch.stack(crops, dim=1)\n    crop = crop.view(B // num_crops, num_crops, crop.size(1), crop.size(2), crop.size(3))\n\n    return crop","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.217317Z","iopub.execute_input":"2023-11-23T16:48:49.217703Z","iopub.status.idle":"2023-11-23T16:48:49.230760Z","shell.execute_reply.started":"2023-11-23T16:48:49.217672Z","shell.execute_reply":"2023-11-23T16:48:49.230011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BaseModel","metadata":{}},{"cell_type":"code","source":"class BaseModel(torch.nn.Module):\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n        self.device = torch.device('cuda:0') if opt.num_gpus > 0 else torch.device('cpu')\n\n    def initialize(self):\n        pass\n\n    def per_gpu_initialize(self):\n        pass\n\n    def compute_generator_losses(self, data_i):\n        return {}\n\n    def compute_discriminator_losses(self, data_i):\n        return {}\n\n    def get_visuals_for_snapshot(self, data_i):\n        return {}\n\n    def get_parameters_for_mode(self, mode):\n        return {}\n\n    def save(self, total_steps_so_far):\n        savedir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        checkpoint_name = \"%dk_checkpoint.pth\" % (total_steps_so_far // 1000)\n        savepath = os.path.join(savedir, checkpoint_name)\n        torch.save(self.state_dict(), savepath)\n        sympath = os.path.join(savedir, \"latest_checkpoint.pth\")\n        if os.path.exists(sympath):\n            os.remove(sympath)\n        os.symlink(checkpoint_name, sympath)\n\n    def load(self):\n        if self.opt.isTrain and self.opt.pretrained_name is not None:\n            loaddir = os.path.join(self.opt.checkpoints_dir, self.opt.pretrained_name)\n        else:\n            loaddir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        checkpoint_name = \"%s_checkpoint.pth\" % self.opt.resume_iter\n        checkpoint_path = os.path.join(loaddir, checkpoint_name)\n        if not os.path.exists(checkpoint_path):\n            print(\"\\n\\ncheckpoint %s does not exist!\" % checkpoint_path)\n            assert self.opt.isTrain, \"In test mode, the checkpoint file must exist\"\n            print(\"Training will start from scratch\")\n            return\n        state_dict = torch.load(checkpoint_path,\n                                map_location=str(self.device))\n        # self.load_state_dict(state_dict)\n        own_state = self.state_dict()\n        skip_all = False\n        for name, own_param in own_state.items():\n            if not self.opt.isTrain and (name.startswith(\"D.\") or name.startswith(\"Dpatch.\")):\n                continue\n            if name not in state_dict:\n                print(\"Key %s does not exist in checkpoint. Skipping...\" % name)\n                continue\n            # if name.startswith(\"C.net\"):\n            #    continue\n            param = state_dict[name]\n            if own_param.shape != param.shape:\n                message = \"Key [%s]: Shape does not match the created model (%s) and loaded checkpoint (%s)\" % (name, str(own_param.shape), str(param.shape))\n                if skip_all:\n                    print(message)\n                    min_shape = [min(s1, s2) for s1, s2 in zip(own_param.shape, param.shape)]\n                    ms = min_shape\n                    if len(min_shape) == 1:\n                        own_param[:ms[0]].copy_(param[:ms[0]])\n                        own_param[ms[0]:].copy_(own_param[ms[0]:] * 0)\n                    elif len(min_shape) == 2:\n                        own_param[:ms[0], :ms[1]].copy_(param[:ms[0], :ms[1]])\n                        own_param[ms[0]:, ms[1]:].copy_(own_param[ms[0]:, ms[1]:] * 0)\n                    elif len(ms) == 4:\n                        own_param[:ms[0], :ms[1], :ms[2], :ms[3]].copy_(param[:ms[0], :ms[1], :ms[2], :ms[3]])\n                        own_param[ms[0]:, ms[1]:, ms[2]:, ms[3]:].copy_(own_param[ms[0]:, ms[1]:, ms[2]:, ms[3]:] * 0)\n                    else:\n                        print(\"Skipping min_shape of %s\" % str(ms))\n                    continue\n                userinput = input(\"%s. Force loading? (yes, no, all) \" % (message))\n                if userinput.lower() == \"yes\":\n                    pass\n                elif userinput.lower() == \"no\":\n                    #assert own_param.shape == param.shape\n                    continue\n                elif userinput.lower() == \"all\":\n                    skip_all = True\n                else:\n                    raise ValueError(userinput)\n                min_shape = [min(s1, s2) for s1, s2 in zip(own_param.shape, param.shape)]\n                ms = min_shape\n                if len(min_shape) == 1:\n                    own_param[:ms[0]].copy_(param[:ms[0]])\n                    own_param[ms[0]:].copy_(own_param[ms[0]:] * 0)\n                elif len(min_shape) == 2:\n                    own_param[:ms[0], :ms[1]].copy_(param[:ms[0], :ms[1]])\n                    own_param[ms[0]:, ms[1]:].copy_(own_param[ms[0]:, ms[1]:] * 0)\n                elif len(ms) == 4:\n                    own_param[:ms[0], :ms[1], :ms[2], :ms[3]].copy_(param[:ms[0], :ms[1], :ms[2], :ms[3]])\n                    own_param[ms[0]:, ms[1]:, ms[2]:, ms[3]:].copy_(own_param[ms[0]:, ms[1]:, ms[2]:, ms[3]:] * 0)\n                else:\n                    print(\"Skipping min_shape of %s\" % str(ms))\n                continue\n            own_param.copy_(param)\n        print(\"checkpoint loaded from %s\" % os.path.join(loaddir, checkpoint_name))\n\n    def forward(self, *args, command=None, **kwargs):\n        \"\"\" wrapper for multigpu training. BaseModel is expected to be\n        wrapped in nn.parallel.DataParallel, which distributes its call to\n        the BaseModel instance on each GPU \"\"\"\n        if command is not None:\n            method = getattr(self, command)\n            assert callable(method), \"[%s] is not a method of %s\" % (command, type(self).__name__)\n            return method(*args, **kwargs)\n        else:\n            raise ValueError(command)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.231874Z","iopub.execute_input":"2023-11-23T16:48:49.232123Z","iopub.status.idle":"2023-11-23T16:48:49.261098Z","shell.execute_reply.started":"2023-11-23T16:48:49.232101Z","shell.execute_reply":"2023-11-23T16:48:49.259982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"import torchvision\n\ndef gan_loss(pred, should_be_classified_as_real):\n    bs = pred.size(0)\n    if should_be_classified_as_real:\n        return F.softplus(-pred).view(bs, -1).mean(dim=1)\n    else:\n        return F.softplus(pred).view(bs, -1).mean(dim=1)\n\n\ndef feature_matching_loss(xs, ys, equal_weights=False, num_layers=6):\n    loss = 0.0\n    for i, (x, y) in enumerate(zip(xs[:num_layers], ys[:num_layers])):\n        if equal_weights:\n            weight = 1.0 / min(num_layers, len(xs))\n        else:\n            weight = 1 / (2 ** (min(num_layers, len(xs)) - i))\n        loss = loss + (x - y).abs().flatten(1).mean(1) * weight\n    return loss\n\n\nclass IntraImageNCELoss(nn.Module):\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean')\n\n    def forward(self, query, target):\n        num_locations = min(query.size(2) * query.size(3), self.opt.intraimage_num_locations)\n        bs = query.size(0)\n        patch_ids = torch.randperm(num_locations, device=query.device)\n\n        query = query.flatten(2, 3)\n        target = target.flatten(2, 3)\n\n        # both query and target are of size B x C x N\n        query = query[:, :, patch_ids]\n        target = target[:, :, patch_ids]\n\n        cosine_similarity = torch.bmm(query.transpose(1, 2), target)\n        cosine_similarity = cosine_similarity.flatten(0, 1)\n        target_label = torch.arange(num_locations, dtype=torch.long, device=query.device).repeat(bs)\n        loss = self.cross_entropy_loss(cosine_similarity / 0.07, target_label)\n        return loss\n\n\nclass VGG16Loss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vgg_convs = torchvision.models.vgg16(pretrained=True).features\n        self.register_buffer('mean',\n                             torch.tensor([0.485, 0.456, 0.406])[None, :, None, None] - 0.5)\n        self.register_buffer('stdev',\n                             torch.tensor([0.229, 0.224, 0.225])[None, :, None, None] * 2)\n        self.downsample = Downsample([1, 2, 1], factor=2)\n\n    def copy_section(self, source, start, end):\n        slice = torch.nn.Sequential()\n        for i in range(start, end):\n            slice.add_module(str(i), source[i])\n        return slice\n\n    def vgg_forward(self, x):\n        x = (x - self.mean) / self.stdev\n        features = []\n        for name, layer in self.vgg_convs.named_children():\n            if \"MaxPool2d\" == type(layer).__name__:\n                features.append(x)\n                if len(features) == 3:\n                    break\n                x = self.downsample(x)\n            else:\n                x = layer(x)\n        return features\n\n    def forward(self, x, y):\n        y = y.detach()\n        loss = 0\n        weights = [1 / 32, 1 / 16, 1 / 8, 1 / 4, 1.0]\n        #weights = [1] * 5\n        total_weights = 0.0\n        for i, (xf, yf) in enumerate(zip(self.vgg_forward(x), self.vgg_forward(y))):\n            loss += F.l1_loss(xf, yf) * weights[i]\n            total_weights += weights[i]\n        return loss / total_weights\n\n\nclass NCELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean')\n\n    def forward(self, query, target, negatives):\n        query = normalize(query.flatten(1))\n        target = normalize(target.flatten(1))\n        negatives = normalize(negatives.flatten(1))\n        bs = query.size(0)\n        sim_pos = (query * target).sum(dim=1, keepdim=True)\n        sim_neg = torch.mm(query, negatives.transpose(0, 1))\n        all_similarity = torch.cat([sim_pos, sim_neg], axis=1) / 0.07\n        #sim_target = util.compute_similarity_logit(query, target)\n        #sim_target = torch.mm(query, target.transpose(0, 1)) / 0.07\n        #sim_query = util.compute_similarity_logit(query, query)\n        #util.set_diag_(sim_query, -20.0)\n\n        #all_similarity = torch.cat([sim_target, sim_query], axis=1)\n\n        #target_label = torch.arange(bs, dtype=torch.long,\n        #                            device=query.device)\n        target_label = torch.zeros(bs, dtype=torch.long, device=query.device)\n        loss = self.cross_entropy_loss(all_similarity,\n                                       target_label)\n        return loss\n\n\nclass ScaleInvariantReconstructionLoss(nn.Module):\n    def forward(self, query, target):\n        query_flat = query.transpose(1, 3)\n        target_flat = target.transpose(1, 3)\n        dist = 1.0 - torch.bmm(\n            query_flat[:, :, :, None, :].flatten(0, 2),\n            target_flat[:, :, :, :, None].flatten(0, 2),\n        )\n\n        target_spatially_flat = target.flatten(1, 2)\n        num_samples = min(target_spatially_flat.size(1), 64)\n        random_indices = torch.randperm(num_samples, dtype=torch.long, device=target.device)\n        randomly_sampled = target_spatially_flat[:, random_indices]\n        random_indices = torch.randperm(num_samples, dtype=torch.long, device=target.device)\n        another_random_sample = target_spatially_flat[:, random_indices]\n\n        random_similarity = torch.bmm(\n            randomly_sampled[:, :, None, :].flatten(0, 1),\n            torch.flip(another_random_sample, [0])[:, :, :, None].flatten(0, 1)\n        )\n\n        return dist.mean() + random_similarity.clamp(min=0.0).mean()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.263023Z","iopub.execute_input":"2023-11-23T16:48:49.263469Z","iopub.status.idle":"2023-11-23T16:48:49.291962Z","shell.execute_reply.started":"2023-11-23T16:48:49.263433Z","shell.execute_reply":"2023-11-23T16:48:49.291009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Swapping Autoencoder Model","metadata":{}},{"cell_type":"code","source":"class SwappingAutoencoderModel(BaseModel):\n    \n    def initialize(self):\n        self.E = Encoder(opt)\n        self.G = Generator(opt)\n        if self.opt.lambda_GAN > 0.0:\n            self.D = Discriminator(opt)\n        if self.opt.lambda_PatchGAN > 0.0:\n            self.Dpatch = PatchDiscriminator(opt)\n\n        # Count the iteration count of the discriminator\n        # Used for lazy R1 regularization (c.f. Appendix B of StyleGAN2)\n        self.register_buffer(\n            \"num_discriminator_iters\", torch.zeros(1, dtype=torch.long)\n        )\n        self.l1_loss = torch.nn.L1Loss()\n\n#         if (not self.opt.isTrain) or self.opt.continue_train:\n#             self.load()\n\n        if self.opt.num_gpus > 0:\n            self.to(\"cuda:0\")\n\n    def per_gpu_initialize(self):\n        pass\n    \n    def get_random_crops(self, x):\n        \"\"\" Make random crops.\n            Corresponds to the yellow and blue random crops of Figure 2.\n        \"\"\"\n        crops = apply_random_crop(\n            x, self.opt.patch_size,\n            (self.opt.patch_min_scale, self.opt.patch_max_scale),\n            num_crops=self.opt.patch_num_crops\n        )\n        return crops\n\n    def swap(self, x):\n        \"\"\" Swaps (or mixes) the ordering of the minibatch to achieve transfer\"\"\"\n        shape = x.shape\n        assert shape[0] % 2 == 0, \"Minibatch size must be a multiple of 2\"\n        new_shape = [shape[0] // 2, 2] + list(shape[1:])\n        x = x.view(*new_shape)\n        x = torch.flip(x, [1])\n        return x.view(*shape)\n\n    def compute_image_discriminator_losses(self, real, rec, mix):\n#         if self.opt.lambda_GAN == 0.0:\n#             return {}\n\n        pred_real = self.D(real)\n        pred_rec = self.D(rec)\n        pred_mix = self.D(mix)\n\n        losses = {}\n        losses[\"D_real\"] = gan_loss(\n            pred_real, should_be_classified_as_real=True\n        ) * self.opt.lambda_GAN\n\n        losses[\"D_rec\"] = gan_loss(\n            pred_rec, should_be_classified_as_real=False\n        ) * (0.5 * self.opt.lambda_GAN)\n        losses[\"D_mix\"] = gan_loss(\n            pred_mix, should_be_classified_as_real=False\n        ) * (0.5 * self.opt.lambda_GAN)\n\n        return losses\n\n    def compute_patch_discriminator_losses(self, real, mix):\n        losses = {}\n        real_feat = self.Dpatch.extract_features(\n            self.get_random_crops(real),\n            aggregate=self.opt.patch_use_aggregation\n        )\n        target_feat = self.Dpatch.extract_features(self.get_random_crops(real))\n        mix_feat = self.Dpatch.extract_features(self.get_random_crops(mix))\n\n        losses[\"PatchD_real\"] = gan_loss(\n            self.Dpatch.discriminate_features(real_feat, target_feat),\n            should_be_classified_as_real=True,\n        ) * self.opt.lambda_PatchGAN\n\n        losses[\"PatchD_mix\"] = gan_loss(\n            self.Dpatch.discriminate_features(real_feat, mix_feat),\n            should_be_classified_as_real=False,\n        ) * self.opt.lambda_PatchGAN\n\n        return losses\n\n    def compute_discriminator_losses(self, real):\n        self.num_discriminator_iters.add_(1)\n\n        sp, gl = self.E(real)\n        B = real.size(0)\n        assert B % 2 == 0, \"Batch size must be even on each GPU.\"\n\n        # To save memory, compute the GAN loss on only\n        # half of the reconstructed images\n        rec = self.G(sp[:B // 2], gl[:B // 2])\n        mix = self.G(self.swap(sp), gl)\n\n        losses = self.compute_image_discriminator_losses(real, rec, mix)\n\n        if self.opt.lambda_PatchGAN > 0.0:\n            patch_losses = self.compute_patch_discriminator_losses(real, mix)\n            losses.update(patch_losses)\n\n        metrics = {}  # no metrics to report for the Discriminator iteration\n\n        return losses, metrics, sp.detach(), gl.detach()\n\n    def compute_R1_loss(self, real):\n        losses = {}\n        if self.opt.lambda_R1 > 0.0:\n            real.requires_grad_()\n            pred_real = self.D(real).sum()\n            grad_real, = torch.autograd.grad(\n                outputs=pred_real,\n                inputs=[real],\n                create_graph=True,\n                retain_graph=True,\n            )\n            grad_real2 = grad_real.pow(2)\n            dims = list(range(1, grad_real2.ndim))\n            grad_penalty = grad_real2.sum(dims) * (self.opt.lambda_R1 * 0.5)\n        else:\n            grad_penalty = 0.0\n\n        if self.opt.lambda_patch_R1 > 0.0:\n            real_crop = self.get_random_crops(real).detach()\n            real_crop.requires_grad_()\n            target_crop = self.get_random_crops(real).detach()\n            target_crop.requires_grad_()\n\n            real_feat = self.Dpatch.extract_features(\n                real_crop,\n                aggregate=self.opt.patch_use_aggregation)\n            target_feat = self.Dpatch.extract_features(target_crop)\n            pred_real_patch = self.Dpatch.discriminate_features(\n                real_feat, target_feat\n            ).sum()\n\n            grad_real, grad_target = torch.autograd.grad(\n                outputs=pred_real_patch,\n                inputs=[real_crop, target_crop],\n                create_graph=True,\n                retain_graph=True,\n            )\n\n            dims = list(range(1, grad_real.ndim))\n            grad_crop_penalty = grad_real.pow(2).sum(dims) + grad_target.pow(2).sum(dims)\n            grad_crop_penalty *= (0.5 * self.opt.lambda_patch_R1 * 0.5)\n        else:\n            grad_crop_penalty = 0.0\n\n        losses[\"D_R1\"] = grad_penalty + grad_crop_penalty\n\n        return losses\n\n    def compute_generator_losses(self, real, sp_ma, gl_ma):\n        losses, metrics = {}, {}\n        B = real.size(0)\n\n        sp, gl = self.E(real)\n        rec = self.G(sp[:B // 2], gl[:B // 2])  # only on B//2 to save memory\n        sp_mix = self.swap(sp)\n        \n        # record the error of the reconstructed images for monitoring purposes\n        metrics[\"L1_dist\"] = self.l1_loss(rec, real[:B // 2])\n\n        if self.opt.lambda_L1 > 0.0:\n            losses[\"G_L1\"] = metrics[\"L1_dist\"] * self.opt.lambda_L1\n\n        if self.opt.crop_size >= 1024:\n            # another momery-saving trick: reduce #outputs to save memory\n            real = real[B // 2:]\n            gl = gl[B // 2:]\n            sp_mix = sp_mix[B // 2:]\n\n        mix = self.G(sp_mix, gl)\n\n        if self.opt.lambda_GAN > 0.0:\n            losses[\"G_GAN_rec\"] = gan_loss(\n                self.D(rec),\n                should_be_classified_as_real=True\n            ) * (self.opt.lambda_GAN * 0.5)\n\n            losses[\"G_GAN_mix\"] = gan_loss(\n                self.D(mix),\n                should_be_classified_as_real=True\n            ) * (self.opt.lambda_GAN * 1.0)\n\n        if self.opt.lambda_PatchGAN > 0.0:\n            real_feat = self.Dpatch.extract_features(\n                self.get_random_crops(real),\n                aggregate=self.opt.patch_use_aggregation).detach()\n            mix_feat = self.Dpatch.extract_features(self.get_random_crops(mix))\n\n            losses[\"G_mix\"] = gan_loss(\n                self.Dpatch.discriminate_features(real_feat, mix_feat),\n                should_be_classified_as_real=True,\n            ) * self.opt.lambda_PatchGAN\n\n        return losses, metrics\n\n    def get_visuals_for_snapshot(self, real):\n        if self.opt.isTrain:\n            # avoid the overhead of generating too many visuals during training\n            real = real[:2] if self.opt.num_gpus > 1 else real[:4]\n        sp, gl = self.E(real)\n        layout = resize2d_tensor(visualize_spatial_code(sp), real)\n        rec = self.G(sp, gl)\n        mix = self.G(sp, self.swap(gl))\n\n        visuals = {\"real\": real, \"layout\": layout, \"rec\": rec, \"mix\": mix}\n\n        return visuals\n\n    def fix_noise(self, sample_image=None):\n        if sample_image is not None:\n            # The generator should be run at least once,\n            # so that the noise dimensions could be computed\n            sp, gl = self.E(sample_image)\n            self.G(sp, gl)\n        noise_var = self.G.fix_and_gather_noise_parameters()\n        return noise_var\n\n    def encode(self, image, extract_features=False):\n        return self.E(image, extract_features=extract_features)\n\n    def decode(self, spatial_code, global_code):\n        return self.G(spatial_code, global_code)\n\n    def get_parameters_for_mode(self, mode):\n        if mode == \"generator\":\n            return list(self.G.parameters()) + list(self.E.parameters())\n        elif mode == \"discriminator\":\n            Dparams = []\n            if self.opt.lambda_GAN > 0.0:\n                Dparams += list(self.D.parameters())\n            if self.opt.lambda_PatchGAN > 0.0:\n                Dparams += list(self.Dpatch.parameters())\n            return Dparams","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.293373Z","iopub.execute_input":"2023-11-23T16:48:49.293666Z","iopub.status.idle":"2023-11-23T16:48:49.330448Z","shell.execute_reply.started":"2023-11-23T16:48:49.293643Z","shell.execute_reply":"2023-11-23T16:48:49.329562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"class MultiGPUModelWrapper():\n    def __init__(self, opt, model: BaseModel):\n        self.opt = opt\n        if opt.num_gpus > 0:\n            model = model.to('cuda:0')\n        self.parallelized_model = torch.nn.parallel.DataParallel(model)\n        self.parallelized_model(command=\"per_gpu_initialize\")\n        self.singlegpu_model = self.parallelized_model.module\n        self.singlegpu_model(command=\"per_gpu_initialize\")\n\n    def get_parameters_for_mode(self, mode):\n        return self.singlegpu_model.get_parameters_for_mode(mode)\n\n    def save(self, total_steps_so_far):\n        self.singlegpu_model.save(total_steps_so_far)\n\n    def __call__(self, *args, **kwargs):\n        \"\"\" Calls are forwarded to __call__ of BaseModel through DataParallel, and corresponding methods specified by |command| will be called. Please see BaseModel.forward() to see how it is done. \"\"\"\n        return self.parallelized_model(*args, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.332086Z","iopub.execute_input":"2023-11-23T16:48:49.332429Z","iopub.status.idle":"2023-11-23T16:48:49.345490Z","shell.execute_reply.started":"2023-11-23T16:48:49.332397Z","shell.execute_reply":"2023-11-23T16:48:49.344574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseOptimizer():\n    def __init__(self, model: MultiGPUModelWrapper):\n        self.opt = model.opt\n\n    def train_one_step(self, data_i, total_steps_so_far):\n        pass\n\n    def get_visuals_for_snapshot(self, data_i):\n        return {}\n\n    def save(self, total_steps_so_far):\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.346636Z","iopub.execute_input":"2023-11-23T16:48:49.346922Z","iopub.status.idle":"2023-11-23T16:48:49.358929Z","shell.execute_reply.started":"2023-11-23T16:48:49.346894Z","shell.execute_reply":"2023-11-23T16:48:49.358123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SwappingAutoencoderOptimizer(BaseOptimizer):\n    \n    def __init__(self, model: MultiGPUModelWrapper):\n        self.opt = model.opt\n        opt = self.opt\n        self.model = model\n        self.train_mode_counter = 0\n        self.discriminator_iter_counter = 0\n\n        self.Gparams = self.model.get_parameters_for_mode(\"generator\")\n        self.Dparams = self.model.get_parameters_for_mode(\"discriminator\")\n\n        self.optimizer_G = torch.optim.Adam(\n            self.Gparams, lr=opt.lr, betas=(opt.beta1, opt.beta2)\n        )\n\n        # c.f. StyleGAN2 (https://arxiv.org/abs/1912.04958) Appendix B\n        c = opt.R1_once_every / (1 + opt.R1_once_every)\n        self.optimizer_D = torch.optim.Adam(\n            self.Dparams, lr=opt.lr * c, betas=(opt.beta1 ** c, opt.beta2 ** c)\n        )\n\n    def set_requires_grad(self, params, requires_grad):\n        \"\"\" For more efficient optimization, turn on and off\n            recording of gradients for |params|.\n        \"\"\"\n        for p in params:\n            p.requires_grad_(requires_grad)\n\n    def toggle_training_mode(self):\n        modes = [\"discriminator\", \"generator\", \"generator\"]\n        self.train_mode_counter = (self.train_mode_counter + 1) % len(modes)\n        return modes[self.train_mode_counter]\n\n    def train_one_step(self, images_minibatch):\n        if self.toggle_training_mode() == \"generator\":\n            losses = self.train_discriminator_one_step(images_minibatch)\n        else:\n            losses = self.train_generator_one_step(images_minibatch)\n        return to_numpy(losses)\n\n    def train_generator_one_step(self, images):\n        self.set_requires_grad(self.Dparams, False)\n        self.set_requires_grad(self.Gparams, True)\n        sp_ma, gl_ma = None, None\n        self.optimizer_G.zero_grad()\n        g_losses, g_metrics = self.model(\n            images, sp_ma, gl_ma, command=\"compute_generator_losses\"\n        )\n        g_loss = sum([v.mean() for v in g_losses.values()])\n        g_loss.backward()\n        self.optimizer_G.step()\n        g_losses.update(g_metrics)\n        return g_losses\n\n    def train_discriminator_one_step(self, images):\n        if self.opt.lambda_GAN == 0.0 and self.opt.lambda_PatchGAN == 0.0:\n            return {}\n        self.set_requires_grad(self.Dparams, True)\n        self.set_requires_grad(self.Gparams, False)\n        self.discriminator_iter_counter += 1\n        self.optimizer_D.zero_grad()\n        d_losses, d_metrics, sp, gl = self.model(\n            images, command=\"compute_discriminator_losses\"\n        )\n        self.previous_sp = sp.detach()\n        self.previous_gl = gl.detach()\n        d_loss = sum([v.mean() for v in d_losses.values()])\n        d_loss.backward()\n        self.optimizer_D.step()\n\n        needs_R1 = self.opt.lambda_R1 > 0.0 or self.opt.lambda_patch_R1 > 0.0\n        needs_R1_at_current_iter = needs_R1 and self.discriminator_iter_counter % self.opt.R1_once_every == 0\n        if needs_R1_at_current_iter:\n            self.optimizer_D.zero_grad()\n            r1_losses = self.model(images, command=\"compute_R1_loss\")\n            d_losses.update(r1_losses)\n            r1_loss = sum([v.mean() for v in r1_losses.values()])\n            r1_loss = r1_loss * self.opt.R1_once_every\n            r1_loss.backward()\n            self.optimizer_D.step()\n\n        d_losses[\"D_total\"] = sum([v.mean() for v in d_losses.values()])\n        d_losses.update(d_metrics)\n        return d_losses\n\n    def get_visuals_for_snapshot(self, data_i):\n        images = self.prepare_images(data_i)\n        with torch.no_grad():\n            return self.model(images, command=\"get_visuals_for_snapshot\")\n\n    def save(self, total_steps_so_far):\n        self.model.save(total_steps_so_far)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.360200Z","iopub.execute_input":"2023-11-23T16:48:49.360463Z","iopub.status.idle":"2023-11-23T16:48:49.378938Z","shell.execute_reply.started":"2023-11-23T16:48:49.360440Z","shell.execute_reply":"2023-11-23T16:48:49.378110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters()) / 1000000","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.380030Z","iopub.execute_input":"2023-11-23T16:48:49.380319Z","iopub.status.idle":"2023-11-23T16:48:49.393146Z","shell.execute_reply.started":"2023-11-23T16:48:49.380295Z","shell.execute_reply":"2023-11-23T16:48:49.392310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base = BaseModel(opt)\nmodel = SwappingAutoencoderModel(opt)\nmodel.initialize()\nparam = torch.load(\"/kaggle/input/ffhq-670k/ffhq_670k.pth\")\nmodel.load_state_dict(param)\noptimizer = SwappingAutoencoderOptimizer(model)\n\nprint(f\"Number of Encoder parameters:             {count_parameters(model.E):.2f}M\")\nprint(f\"Number of Generator parameters:           {count_parameters(model.G):.2f}M\")\nprint(f\"Number of Discriminator parameters:       {count_parameters(model.D):.2f}M\")\nprint(f\"Number of Patch Discriminator parameters: {count_parameters(model.Dpatch):.2f}M\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:49.394200Z","iopub.execute_input":"2023-11-23T16:48:49.394734Z","iopub.status.idle":"2023-11-23T16:48:57.738408Z","shell.execute_reply.started":"2023-11-23T16:48:49.394702Z","shell.execute_reply":"2023-11-23T16:48:57.737447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"class MetricTracker:\n    def __init__(self, opt):\n        self.opt = opt\n        self.metrics = {}\n\n    def moving_average(self, old, new):\n        s = 0.98\n        return old * (s) + new * (1 - s)\n\n    def update_metrics(self, metric_dict, smoothe=True):\n        default_smoothe = smoothe\n        for k, v in metric_dict.items():\n            if k == \"D_R1\":\n                smoothe = False\n            else:\n                smoothe = default_smoothe\n            if k in self.metrics and smoothe:\n                self.metrics[k] = self.moving_average(self.metrics[k], v)\n            else:\n                self.metrics[k] = v\n\n    def current_metrics(self):\n        keys = sorted(list(self.metrics.keys()))\n        ordered_metrics = OrderedDict([(k, self.metrics[k]) for k in keys])\n        return ordered_metrics\n    \nmetric_tracker = MetricTracker(opt)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:57.739678Z","iopub.execute_input":"2023-11-23T16:48:57.740057Z","iopub.status.idle":"2023-11-23T16:48:57.748536Z","shell.execute_reply.started":"2023-11-23T16:48:57.740024Z","shell.execute_reply":"2023-11-23T16:48:57.747560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_current_losses(iters, losses):\n    \"\"\"\n    Parameters:\n        epoch (int) -- current epoch\n        iters (int) -- current training iteration during this epoch (reset to 0 at the end of every epoch)\n        losses (OrderedDict) -- training losses stored in the format of (name, float) pairs\n        t_comp (float) -- computational time per data point (normalized by batch_size)\n        t_data (float) -- data loading time per data point (normalized by batch_size)\n    \"\"\"\n    message = 'iters: %d, ' % (iters)\n    \n    for k, v in losses.items():\n        message += '%s: %.3f, ' % (k, v.mean())\n    print(message)  # print the message\n    \n    \ndef display_current_results(model, real):\n    sp, gl = model.E(real)\n    rec = model.G(sp, gl)\n    mix = model.G(sp, model.swap(gl))\n    \n    display(to_pil_image(rec[0]))\n    display(to_pil_image(mix[0]))\n    display(to_pil_image(rec[1]))\n    display(to_pil_image(mix[1]))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:57.749744Z","iopub.execute_input":"2023-11-23T16:48:57.750117Z","iopub.status.idle":"2023-11-23T16:48:57.762855Z","shell.execute_reply.started":"2023-11-23T16:48:57.750083Z","shell.execute_reply":"2023-11-23T16:48:57.761952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom torchvision.transforms.functional import to_tensor\n\nepochs = 50\niters = 0\n\nstart_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:57.763946Z","iopub.execute_input":"2023-11-23T16:48:57.764244Z","iopub.status.idle":"2023-11-23T16:48:57.777700Z","shell.execute_reply.started":"2023-11-23T16:48:57.764212Z","shell.execute_reply":"2023-11-23T16:48:57.776715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# anime_path = \"/kaggle/input/test-jin/Test/J (1).png\"\n# human_path = \"/kaggle/input/test-jin/Test/jin (13).png\"\n\n# anime_img = Image.open(anime_path)\n# human_img = Image.open(human_path)\n\n# anime_tensor = torch.unsqueeze(to_tensor(anime_img), 0).to(\"cuda\")\n# human_tensor = torch.unsqueeze(to_tensor(human_img), 0).to(\"cuda\")\n\n# anime_sp, anime_gl = model.E(anime_tensor)\n# human_sp, human_gl = model.E(human_tensor)\n\n# aa_rec = model.G(anime_sp, anime_gl)\n# ah_rec = model.G(anime_sp, human_gl)\n# hh_rec = model.G(human_sp, human_gl)\n# ha_rec = model.G(human_sp, anime_gl)\n\n# display(to_pil_image(aa_rec[0]))\n# display(to_pil_image(ah_rec[0]))\n# display(to_pil_image(hh_rec[0]))\n# display(to_pil_image(ha_rec[0]))\n\n# del anime_sp, anime_gl, human_sp, human_gl, aa_rec, ah_rec, hh_rec, ha_rec","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:57.778918Z","iopub.execute_input":"2023-11-23T16:48:57.779218Z","iopub.status.idle":"2023-11-23T16:48:57.788209Z","shell.execute_reply.started":"2023-11-23T16:48:57.779194Z","shell.execute_reply":"2023-11-23T16:48:57.787438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    for batch in Cartoon_Dataloader:\n        img_batch = [Image.open(img) for tuple in zip(*batch) for img in tuple]\n        for i in range(batch_size*2):\n            img_batch[i] = to_tensor(img_batch[i])\n        \n        img_batch = torch.stack(img_batch).to('cuda')\n        \n        losses = optimizer.train_one_step(img_batch)\n        metric_tracker.update_metrics(losses, smoothe=True)\n        \n        iters += 1\n        if iters % opt.print_freq == 0:\n            print_current_losses(iters, metric_tracker.current_metrics())\n        \n        if iters % opt.display_freq == 0:\n            display_current_results(model, img_batch)\n            print(round((time.time() - start_time) / 60), 2)\n            \n        if iters % opt.save_freq == 0:\n            torch.save(model.state_dict(), \"./ffhq_690k.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:48:57.789240Z","iopub.execute_input":"2023-11-23T16:48:57.789797Z"},"trusted":true},"execution_count":null,"outputs":[]}]}